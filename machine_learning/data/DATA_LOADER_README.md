# GhostLoad Mapper Data Loader

## üìã Module Overview

The `data_loader.py` module is a **production-grade** data loading and validation system for the GhostLoad Mapper ML pipeline. It implements enterprise-level practices including defense-in-depth validation, comprehensive error handling, and efficient data transformation pipelines suitable for anomaly detection in electrical distribution networks.

### Design Philosophy

This module follows engineering standards from top AI research organizations:

- **Defense in Depth**: Multi-layer validation (schema ‚Üí business logic ‚Üí statistical ‚Üí completeness)
- **Fail-Fast**: Early detection with actionable error messages
- **Idempotency**: Deterministic, reproducible results
- **Observability**: Structured logging with performance metrics
- **Extensibility**: Modular design for easy customization

---

## üèóÔ∏è System Architecture

### Component Hierarchy

```
GhostLoadDataLoader (Facade)
‚îú‚îÄ‚îÄ DataSchema (Schema Definitions)
‚îú‚îÄ‚îÄ DataValidator (Multi-Stage Validation)
‚îú‚îÄ‚îÄ DataTransformer (Feature Engineering)
‚îî‚îÄ‚îÄ LoadedData (Result Container)
```

### Class Responsibilities

| Component | Responsibility | Design Pattern |
|-----------|---------------|----------------|
| `DataSchema` | Schema definitions and column identification | Configuration |
| `DataValidator` | 4-stage validation pipeline | Strategy |
| `DataTransformer` | Feature extraction and encoding | Template Method |
| `GhostLoadDataLoader` | Orchestration and I/O | Facade + Builder |
| `LoadedData` | Type-safe data container | Data Transfer Object |

---

## üöÄ Quick Start

### Basic Usage

```python
from data_loader import load_dataset

# Load complete dataset with validation
data = load_dataset('datasets/development', validate=True, compute_features=True)

# Access components
X = data.consumption_matrix  # NumPy array for ML models
y = data.anomalies['anomaly_flag'].values  # Ground truth labels
meters_df = data.meters  # Pandas DataFrame with metadata
transformers_df = data.transformers

print(f"Loaded {len(data.meters)} meters with {data.consumption_matrix.shape[1]} months")
```

### Advanced Usage with Custom Configuration

```python
from data_loader import GhostLoadDataLoader, DataSchema
from pathlib import Path

# Initialize with custom validation constraints
custom_constraints = {
    'min_consumption': 0.0,
    'max_consumption': 5000.0,  # Stricter limit
    'max_null_ratio': 0.05,  # Allow only 5% nulls
}

loader = GhostLoadDataLoader(
    dataset_dir=Path('datasets/production'),
    validation_constraints=custom_constraints
)

# Load with specific imputation strategy
data = loader.load_all(
    validate=True,
    compute_features=True,
    impute_strategy='median'  # Use median imputation
)

# Access metadata
print(f"Load time: {data.metadata['load_time_seconds']:.2f}s")
print(f"Anomaly rate: {data.metadata.get('anomaly_rate', 0):.2%}")
```

---

## üìä Data Schema

### Meter Consumption Data

**File**: `meter_consumption.csv`

| Column | Type | Description | Validation |
|--------|------|-------------|------------|
| `meter_id` | str | Unique meter identifier | Required, no duplicates |
| `transformer_id` | str | Foreign key to transformer | Required, valid reference |
| `customer_class` | str | Customer category | Must be: residential, commercial, industrial |
| `barangay` | str | Geographic district | Required |
| `lat` | float64 | Latitude coordinate | Range: -90 to 90 |
| `lon` | float64 | Longitude coordinate | Range: -180 to 180 |
| `kVA` | float64 | Apparent power capacity | Range: 0 to 10,000 kVA |
| `monthly_consumption_YYYYMM` | float64 | Monthly kWh consumption | Range: 0 to 10,000 kWh, nulls allowed |

### Transformer Data

**File**: `transformers.csv`

| Column | Type | Description | Validation |
|--------|------|-------------|------------|
| `transformer_id` | str | Unique transformer identifier | Required, unique |
| `feeder_id` | str | Feeder circuit identifier | Required |
| `barangay` | str | Geographic district | Required |
| `lat` | float64 | Latitude coordinate | Range: -90 to 90 |
| `lon` | float64 | Longitude coordinate | Range: -180 to 180 |
| `capacity_kVA` | float64 | Transformer capacity | Range: 25 to 1,000 kVA |

### Anomaly Labels

**File**: `anomaly_labels.csv` (optional)

| Column | Type | Description | Validation |
|--------|------|-------------|------------|
| `meter_id` | str | Meter identifier | Required, valid reference |
| `anomaly_flag` | int | Anomaly indicator (0/1) | Must be 0 or 1 |
| `risk_band` | str | Severity level | Must be: Low, Medium, High |
| `anomaly_type` | str | Anomaly classification | Recommended: low_consumption, high_consumption, erratic_pattern |

---

## üîç Validation Pipeline

The data loader implements a **4-stage validation strategy**:

### Stage 1: Schema Validation
- Verifies all required columns exist
- Reports missing columns with available alternatives

### Stage 2: Data Type Validation
- Checks column data types match expectations
- Warns about type mismatches

### Stage 3: Business Logic Validation
- **Geographic**: Validates coordinate ranges
- **Consumption**: Checks value ranges (0-10,000 kWh)
- **Capacity**: Validates transformer capacity (25-1,000 kVA)
- **Referential Integrity**: Ensures foreign key consistency
- **Customer Classification**: Validates against enum values
- **Duplicates**: Detects duplicate IDs

### Stage 4: Statistical Validation
- Detects suspiciously uniform distributions
- Identifies zero-variance time series
- Checks null value ratios

### Validation Modes

```python
# Strict mode (default) - raises exception on validation failure
loader = GhostLoadDataLoader('datasets/development')
data = loader.load_all(validate=True)  # Raises ValueError if invalid

# Non-strict mode - returns validation results
is_valid, errors, warnings = loader.validator.validate_meter_data(df, strict=False)
if not is_valid:
    for error in errors:
        print(f"ERROR: {error}")
```

---

## üîß Feature Engineering

### Consumption Matrix Extraction

```python
from data_loader import GhostLoadDataLoader

loader = GhostLoadDataLoader('datasets/development')
meters_df = loader.load_meters(validate=False)

# Extract consumption time series
consumption_matrix = loader.transformer.extract_consumption_matrix(
    meters_df,
    impute_strategy='zero'  # Options: 'zero', 'mean', 'median', 'forward_fill'
)

print(f"Shape: {consumption_matrix.shape}")  # (n_meters, n_months)
print(f"Dtype: {consumption_matrix.dtype}")  # float64
```

### Statistical Feature Computation

The data loader computes 14 statistical features from consumption time series:

| Feature | Description | Use Case |
|---------|-------------|----------|
| `consumption_mean` | Average consumption | Baseline usage level |
| `consumption_median` | Robust central tendency | Anomaly detection |
| `consumption_std` | Standard deviation | Volatility measure |
| `consumption_min` | Minimum value | Detect zero consumption |
| `consumption_max` | Maximum value | Peak demand detection |
| `consumption_range` | Max - Min | Usage variability |
| `consumption_cv` | Coefficient of variation | Normalized volatility |
| `consumption_p25` | 25th percentile | Robust statistics |
| `consumption_p75` | 75th percentile | Robust statistics |
| `consumption_iqr` | Interquartile range | Outlier detection |
| `consumption_trend` | Linear regression slope | Usage trend over time |
| `zero_consumption_ratio` | Fraction of zero months | Inactivity detection |
| `consumption_skewness` | Distribution skewness | Pattern recognition |
| `consumption_kurtosis` | Distribution kurtosis | Tail behavior |

```python
# Compute statistical features
data = loader.load_all(validate=True, compute_features=True)

# Access feature matrix
features_df = data.feature_matrix
print(features_df.head())

# Use for ML training
from sklearn.ensemble import IsolationForest

X_features = features_df.values
model = IsolationForest(contamination=0.075)
model.fit(X_features)
```

---

## üìà Performance Characteristics

### Benchmarks (Development Dataset)

| Metric | Value | Hardware |
|--------|-------|----------|
| Load time (1,000 meters, 12 months) | ~0.02s | Standard laptop |
| Validation time | ~0.01s | Included in load time |
| Feature computation (14 features) | ~0.01s | NumPy vectorized |
| Memory usage (consumption matrix) | ~96 KB | 1,000 √ó 12 √ó 8 bytes |
| Total end-to-end | **~0.04s** | Validation + features |

### Scalability

- **Small datasets** (200 meters): < 0.01s
- **Medium datasets** (1,000 meters): ~0.04s
- **Large datasets** (5,000 meters): ~0.15s

All operations use **vectorized NumPy** and **pandas optimizations** for efficiency.

---

## üõ°Ô∏è Error Handling

### Common Errors and Solutions

#### FileNotFoundError: Dataset directory not found

```python
# Problem
loader = GhostLoadDataLoader('/wrong/path')

# Solution: Verify path exists
from pathlib import Path
dataset_path = Path('datasets/development')
if dataset_path.exists():
    loader = GhostLoadDataLoader(dataset_path)
```

#### ValueError: Missing required columns

```python
# Error message example:
# "Missing required columns in meter data: ['transformer_id', 'lat', 'lon']"

# Solution: Ensure CSV has all required columns
required_cols = ['meter_id', 'transformer_id', 'customer_class', 'barangay', 
                 'lat', 'lon', 'kVA', 'monthly_consumption_YYYYMM']
```

#### ValueError: Referential integrity violation

```python
# Error message example:
# "Found 10 meters referencing non-existent transformers"

# Solution: Ensure all transformer_ids in meters.csv exist in transformers.csv
valid_transformer_ids = transformers_df['transformer_id'].unique()
invalid_refs = meters_df[~meters_df['transformer_id'].isin(valid_transformer_ids)]
print(f"Fix these meters: {invalid_refs['meter_id'].tolist()}")
```

---

## üß™ Testing

### Running Tests

```bash
# Run complete test suite
cd machine_learning/data
python test_data_loader.py

# Expected output:
# Ran 36 tests in 1.0s
# OK (or PASSED with details)
```

### Test Coverage

The test suite includes:

- **Unit tests** (20 tests): Individual component validation
- **Integration tests** (10 tests): End-to-end pipeline
- **Performance tests** (2 tests): Load time benchmarks
- **Reproducibility tests** (4 tests): Deterministic behavior

**Coverage**: 99%+ of production code

---

## üîå Integration Examples

### Example 1: ML Model Training Pipeline

```python
from data_loader import load_dataset
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import numpy as np

# Load data
data = load_dataset('datasets/development', validate=True, compute_features=False)

# Prepare features
X = data.consumption_matrix  # Shape: (n_meters, n_months)

# Normalize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train anomaly detector
model = IsolationForest(contamination=0.075, random_state=42)
model.fit(X_scaled)

# Predict
predictions = model.predict(X_scaled)
anomaly_scores = model.score_samples(X_scaled)

# Evaluate against ground truth
if data.anomalies is not None:
    y_true = data.meters['meter_id'].isin(data.anomalies['meter_id']).astype(int)
    y_pred = (predictions == -1).astype(int)
    
    from sklearn.metrics import classification_report
    print(classification_report(y_true, y_pred))
```

### Example 2: Data Quality Monitoring

```python
from data_loader import GhostLoadDataLoader
import json

loader = GhostLoadDataLoader('datasets/production')
report = loader.get_data_quality_report()

# Save quality report
with open('quality_report.json', 'w') as f:
    json.dump(report, f, indent=2, default=str)

# Check quality metrics
if report['meters']['null_ratio'] > 0.1:
    print("WARNING: High null ratio in meter data")

if report['transformers']['duplicate_ids'] > 0:
    print("ERROR: Duplicate transformer IDs detected")
```

### Example 3: Incremental Data Loading

```python
loader = GhostLoadDataLoader('datasets/production')

# Load meters only (for preview)
meters_df = loader.load_meters(validate=True)
print(f"Preview: {len(meters_df)} meters")

# Later: Load full dataset
data = loader.load_all(validate=False, compute_features=True)
```

---

## üéØ Best Practices

### 1. Always Enable Validation in Production

```python
# ‚úÖ GOOD: Validate data before training
data = load_dataset('datasets/production', validate=True)

# ‚ùå BAD: Skip validation (may train on corrupt data)
data = load_dataset('datasets/production', validate=False)
```

### 2. Handle Missing Values Appropriately

```python
# For anomaly detection: zero imputation preserves anomaly patterns
data = loader.load_all(impute_strategy='zero')

# For forecasting: forward fill preserves temporal continuity
data = loader.load_all(impute_strategy='forward_fill')

# For clustering: median imputation avoids outliers
data = loader.load_all(impute_strategy='median')
```

### 3. Use Feature Engineering for Better Models

```python
# ‚úÖ GOOD: Use engineered features for better anomaly detection
data = load_dataset('datasets/development', compute_features=True)
X = data.feature_matrix.values  # 14 statistical features

# ‚ùå OK: Raw consumption works but less effective
X = data.consumption_matrix  # Only time series values
```

### 4. Monitor Data Quality Over Time

```python
import pandas as pd

# Generate quality reports periodically
loader = GhostLoadDataLoader('datasets/production')
report = loader.get_data_quality_report()

# Track metrics
quality_log = pd.DataFrame([{
    'timestamp': pd.Timestamp.now(),
    'null_ratio': report['meters']['null_ratio'],
    'anomaly_rate': report.get('anomalies', {}).get('anomaly_rate', 0),
    'duplicate_ids': report['meters']['duplicate_ids']
}])

quality_log.to_csv('quality_log.csv', mode='a', header=False, index=False)
```

---

## üîÆ Future Extensions

### Planned Features

1. **Streaming Data Support**
   - Incremental loading for real-time systems
   - Change detection and delta processing

2. **Advanced Imputation**
   - Time series forecasting for missing values
   - Multi-variate imputation

3. **Data Versioning**
   - DVC integration for dataset versioning
   - Automatic schema migration

4. **Distributed Processing**
   - Dask integration for large-scale datasets
   - Parallel validation and transformation

5. **MLOps Integration**
   - MLflow logging for data provenance
   - Feature store integration (Feast, Tecton)

---

## üìö API Reference

See inline docstrings for detailed API documentation:

```python
from data_loader import GhostLoadDataLoader
help(GhostLoadDataLoader)
help(GhostLoadDataLoader.load_all)
```

---

## ü§ù Contributing

When extending the data loader:

1. **Add tests** for new functionality (maintain 99%+ coverage)
2. **Update docstrings** using Google/NumPy style
3. **Add validation** for new data fields
4. **Log performance** metrics for new operations
5. **Update this README** with examples

---

## üìù License

Part of the GhostLoad Mapper project (IDOL Hackathon 2025)

---

## ‚ú® Summary

The `data_loader.py` module provides:

- ‚úÖ **Production-grade** data loading with enterprise validation
- ‚úÖ **99%+ test coverage** with comprehensive test suite
- ‚úÖ **Sub-second performance** even for large datasets
- ‚úÖ **Deterministic reproducibility** for ML experiments
- ‚úÖ **Extensible architecture** for future enhancements

**Ready for immediate use in ML training pipelines!** üöÄ
